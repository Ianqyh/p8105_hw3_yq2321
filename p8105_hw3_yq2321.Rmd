---
title: "P8105 Homework 3"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
data("instacart")
```

The "instacart" dataset contains 1384617 obsevations in total and each row in the dataset represents a product from an order. There are 15 variables in this dataset. The order_id variable is the order identifier. The product_id variable is the product identifier. The add_to cart_order variable represents the order in which each product was added to cart. The reordered variable is 1 if this prodcut has been ordered by this user in the past and 0 otherwise. The user_id variable is the customer identifier. The eval_set variable represents  which evaluation set this order belongs in. The order_number variable is the order sequence number for this user (1 = first,  n = nth). The order_dow variable represents the day of the week on which the order was placed. The order_hour_of_day variable is the hour of the day on which the order was placed. The days_since_prior_order represents days since the last order, capped at 30, NA if order_number = 1. The product_name variable is the name of the product. The aisle_id variable is the aisle identifier. The department_id variable is the department identifier. The aisle variable represents the name of the aisle the product is in. The department variable represents the name of the department the product is in.

```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(total = n()) %>%
  arrange(desc(total))
```

From the table above, we can see that there are 134 aisles and the top 10 aisles that most items are ordered from include fresh vegetables, fresh fruits, packaged vegetable fruits, yogurt, packaged cheese, water seltzer sparkling water, milk, chips pretzels, soy lactosefree, and bread.

```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(total = n()) %>%
  arrange(desc(total)) %>%
  filter(total > 10000) %>%
  mutate(aisle = fct_reorder(aisle, total)) %>%
  ggplot(aes(x = total, y = aisle)) + geom_point() + labs(title = "Top 39 Aisles With Most Items Ordered", xlab = "Number of items", ylab = "Aisle") + scale_x_continuous(breaks = seq(from = 0, to = 160000, by = 20000))
```

The plot above shows the number of items ordered in each aisle.

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(order_times = n()) %>%
  arrange(desc(order_times)) %>%
  mutate(rank = min_rank(desc(order_times))) %>%
  filter(rank <= 3) %>%
  knitr::kable()
```

The table above shows the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables along with the number of times ordered.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_day = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour_day"
  ) %>%
  knitr::kable()
```

The table above shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week (0 represents the first day of the week).


## Problem 2

```{r}
data("brfss_smart2010")
```

```{r}
brfss_smart_cleaned = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = factor(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>%
  separate(locationdesc, into = c("state", "location"), sep = ' - ') %>%
  select(-locationabbr)
```

The above code chunk cleans the data by formatting the data to use appropriate variable names, focusing on the "Overall Health" topic, including only responses from “Excellent” to “Poor”, and organizing responses as a factor taking levels ordered from “Poor” to “Excellent”.

```{r}
brfss_smart_cleaned %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  distinct(location) %>%
  summarize(times = n()) %>%
  filter(times >= 7) %>%
  arrange(times)
```

The result above showed that CT, FL, NC, MA, NJ, and PA were observed at 7 or more locations in 2002.

```{r}
brfss_smart_cleaned %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  distinct(location) %>%
  summarize(times = n()) %>%
  filter(times >= 7) %>%
  arrange(times)
```

The result above showed that CO, PA, SC, OH, MA, NY, NE, WA, CA, MD, NC, TX, NJ, and FL were observed at 7 or more locations in 2010.


```{r}
excellent_dataset = brfss_smart_cleaned %>%
  filter(response == "Excellent") %>%
  group_by(state, year) %>%
  summarize(mean_data_value = mean(data_value))
excellent_dataset
```

The dataset above is limited to Excellent responses, and contains year, state, and a variable averages the data_value across locations within a state.

```{r}
excellent_dataset %>%
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_line(aes(group = state)) + theme(legend.position = "right")
  labs(title = "Average data value of excellent responses across locations within a state",
       y = "Average data value wthin a state")
```

The plot above shows a line of average value across locations for each state over years.


```{r}
NY_data_value_2006 = brfss_smart_cleaned %>%
  filter(year == 2006, state == "NY") %>%
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .4) + theme(legend.position = "none") +
  labs(title = "2006")
NY_data_value_2010 = brfss_smart_cleaned %>%
  filter(year == 2010, state == "NY") %>%
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .4) + theme(legend.position = "right") + labs(title = "2010")
NY_data_value_2006 + NY_data_value_2010
```

The two-panel plot above shows the distribution of data_value for responses among locations in NY state for year 2006 and 2010 respectively.

## Problem 3

```{r}
accelerometer_data = read_csv("hw3 data/accel_data.csv")
```

```{r}
accelerometer_data_final = accelerometer_data %>%
  pivot_longer(cols = activity.1:activity.1440, names_to = "activity number", names_prefix = "activity.", values_to = "activity counts") %>% janitor::clean_names() %>%
  mutate(weekday_or_weekend = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday"))
accelerometer_data_final
```

The resulting dataset contains `r nrow(accelerometer_data_final)` observations and `r ncol(accelerometer_data_final)` variables. The names of these variables are `r names(accelerometer_data_final)`.

```{r}
accelerometer_data_final %>%
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_counts)) %>%
  knitr::kable()
```

The table above shows the total activity for each day. We can see that the total activity generally remains high and stable over time, except for a few days when it drastically decreases, for example, day 2, day 24, and day 31.


```{r}
accelerometer_data_final %>%
  mutate(activity_number_hours = as.numeric(activity_number) / 60) %>%
  ggplot(aes(x = activity_number_hours, y = activity_counts, color = day)) + geom_smooth(se = FALSE) + labs(title = "Average 24-hour activity for each day of the week", x = "Activity number (in hour)", y = "Average activity" )
```
From the plot above, we can see that there are two notable peaks for Thursday and Friday, at 11 am and 9 pm respectively. This indicates that the participant might engage in some activities that demand a lot of physical energies. In addition, the activity tends to be lower at night. This is reasonable as the participant is relatively stationary when he falls asleep. There are little difference in average activity among each day of the week as well as between weekdays and weekends.
